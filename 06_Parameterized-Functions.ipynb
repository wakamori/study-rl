{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to Parameterized Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterizing the Value Function\n",
    "\n",
    "- approximate $v_\\pi(s)$ as $\\hat{v}(s,\\mathbb{w})$\n",
    "\n",
    "$$\n",
    "\\hat{v}(s,\\underbrace{\\mathbb{w}}_{\\text{weights}}) \\approx v_\\pi(s)\n",
    "$$\n",
    "\n",
    "- Example of Parameterized Value Function\n",
    "\n",
    "$$\n",
    "\\hat{v}(s,\\mathbb{w}) \\doteq \\underbrace{w_1 X + w_2 Y}_{\\text{We only have to store the two weights}}\n",
    "$$\n",
    "\n",
    "## Linear Value Function Approximation\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{v}(s,\\mathbb{w}) &\\doteq \\sum{\\underbrace{w_i x_i(s)}_{\\text{features}}}\\\\\n",
    "                      &=<\\mathbb{w},\\mathbb{x}(s)>\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization and Discrimination\n",
    "\n",
    "- Tabular representations have provided good *discrimination*, but no *generalization*\n",
    "- Generalization is important for faster learning\n",
    "- Having both generalization and discrimination is ideal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing Value Estimation as Suprevised Learning\n",
    "\n",
    "- Monte-Carlo\n",
    "\n",
    "$$\n",
    "\\{(S_1,G_1),(S_2,G_2),(S_3,G_3),\\ldots\\}\n",
    "$$\n",
    "\n",
    "- TD\n",
    "\n",
    "$$\n",
    "\\{(S_1,R_2+\\gamma\\hat{v}(S_2,\\mathbb{w})),(S_2,R_3+\\gamma\\hat{v}(S_3,\\mathbb{w})),(S_3,R_4+\\gamma\\hat{v}(S_4,\\mathbb{w})),\\ldots\\}\n",
    "$$\n",
    "\n",
    "## The Function Approximator should be Compatible with Online Updates\n",
    "\n",
    "\n",
    "- We can frame the policy evaluation task as a *suprevised learning* problem\n",
    "- But not all methods from supervised learning are ideal for reinforcement learning\n",
    "  - If we want to use a function approximation technique, we should make sure it can work in the online setting\n",
    "  - The data in reinforcement learning is always correlated\n",
    "\n",
    "## The Function Approximator should be Compatible with Bootstrapping\n",
    "\n",
    "- TD: Target depends on $\\mathbb{w}$\n",
    "- Supervised Learning: Target is fixed and given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mean Squared Value Error Objective\n",
    "\n",
    "- Mean Squared Value Error\n",
    "\n",
    "$$\n",
    "\\sum_{s}{\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbb{w})]^2}\n",
    "$$\n",
    "\n",
    "- How to choose $\\mu(s)$?\n",
    "  - The fraction of time we spend in $S$ when following policy $\\pi$\n",
    "\n",
    "## Adapting the Weights to Minimize the Mean Squared Value Error Objective\n",
    "\n",
    "$$\n",
    "\\overline{VE}=\\sum_{s}{\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbb{w})]^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Gradient Descent\n",
    "\n",
    "- Gradient Descent\n",
    "\n",
    "$$\n",
    "\\mathbb{w}_{t+1} \\doteq \\mathbb{w}_t - \\alpha \\nabla J(\\mathbb{w}_t)\n",
    "$$\n",
    "\n",
    "- Gradient Descent can be used to find stationary points of objectives\n",
    "- These solutions are not always globally optimal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of the Mean Squared Value Error Objective\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\nabla \\sum_{s\\in\\mathcal{S}}{\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbb{w})]^2}\\\\\n",
    "&= \\sum_{s\\in\\mathcal{S}}{\\nabla\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbb{w})]^2}\\\\\n",
    "&= -\\sum_{s\\in\\mathcal{S}}{\\mu(s)2[v_\\pi(s)-\\hat{v}(s,\\mathbb{w})]} \\nabla \\hat{v}(s,\\mathbb{w})& \\text{(chain rule)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\Delta \\mathbb{w} \\propto \\sum_{s\\in\\mathcal{S}}{\\nabla\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbb{w})]}& (\\because\\Delta\\hat{v}(s,\\mathbb{w})=\\mathbb{x}(s))\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Gradient Descent to Stochastic Gradient Descent\n",
    "\n",
    "### Gradient Monte Carlo\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\mathbb{w_{t+1}}\\doteq\\mathbb{w_t}+\\alpha[\\underbrace{v_\\pi(S_t)}_{\\text{?}}-\\hat{v}(S_t,\\mathbb{w})\\nabla\\hat{v}(S_t,\\mathbb{w})]\\\\\n",
    "\\mathbb{w_{t+1}}\\doteq\\mathbb{w_t}+\\alpha[G_t-\\hat{v}(S_t,\\mathbb{w})\\nabla\\hat{v}(S_t,\\mathbb{w})]\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\mathbb{E}_\\pi[2[v_\\pi(S_t)-\\hat{v}(S_t,\\mathbb{w})]\\nabla\\hat{v}(S_t,\\mathbb{w})]\\\\\n",
    "=\\mathbb{E}_\\pi[2[G_t-\\hat{v}(S_t,\\mathbb{w})]\\nabla\\hat{v}(S_t,\\mathbb{w})]\n",
    "\\end{gather*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}