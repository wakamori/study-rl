{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to Parameterized Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterizing the Value Function\n",
    "\n",
    "- approximate $v_\\pi(s)$ as $\\hat{v}(s,\\mathbf{w})$\n",
    "\n",
    "$$\n",
    "\\hat{v}(s,\\underbrace{\\mathbf{w}}_{\\text{weights}}) \\approx v_\\pi(s)\n",
    "$$\n",
    "\n",
    "- Example of Parameterized Value Function\n",
    "\n",
    "$$\n",
    "\\hat{v}(s,\\mathbf{w}) \\doteq \\underbrace{w_1 X + w_2 Y}_{\\text{We only have to store the two weights}}\n",
    "$$\n",
    "\n",
    "## Linear Value Function Approximation\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{v}(s,\\mathbf{w}) &\\doteq \\sum{\\underbrace{w_i x_i(s)}_{\\text{features}}}\\\\\n",
    "                      &=<\\mathbf{w},\\mathbf{x}(s)>\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization and Discrimination\n",
    "\n",
    "- Tabular representations have provided good *discrimination*, but no *generalization*\n",
    "- Generalization is important for faster learning\n",
    "- Having both generalization and discrimination is ideal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing Value Estimation as Suprevised Learning\n",
    "\n",
    "- Monte-Carlo\n",
    "\n",
    "$$\n",
    "\\{(S_1,G_1),(S_2,G_2),(S_3,G_3),\\ldots\\}\n",
    "$$\n",
    "\n",
    "- TD\n",
    "\n",
    "$$\n",
    "\\{(S_1,R_2+\\gamma\\hat{v}(S_2,\\mathbf{w})),(S_2,R_3+\\gamma\\hat{v}(S_3,\\mathbf{w})),(S_3,R_4+\\gamma\\hat{v}(S_4,\\mathbf{w})),\\ldots\\}\n",
    "$$\n",
    "\n",
    "## The Function Approximator should be Compatible with Online Updates\n",
    "\n",
    "\n",
    "- We can frame the policy evaluation task as a *suprevised learning* problem\n",
    "- But not all methods from supervised learning are ideal for reinforcement learning\n",
    "  - If we want to use a function approximation technique, we should make sure it can work in the online setting\n",
    "  - The data in reinforcement learning is always correlated\n",
    "\n",
    "## The Function Approximator should be Compatible with Bootstrapping\n",
    "\n",
    "- TD: Target depends on $\\mathbf{w}$\n",
    "- Supervised Learning: Target is fixed and given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mean Squared Value Error Objective\n",
    "\n",
    "- Mean Squared Value Error\n",
    "\n",
    "$$\n",
    "\\sum_{s}{\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})]^2}\n",
    "$$\n",
    "\n",
    "- How to choose $\\mu(s)$?\n",
    "  - The fraction of time we spend in $S$ when following policy $\\pi$\n",
    "\n",
    "## Adapting the Weights to Minimize the Mean Squared Value Error Objective\n",
    "\n",
    "$$\n",
    "\\overline{VE}=\\sum_{s}{\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})]^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}