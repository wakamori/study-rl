{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to Parameterized Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterizing the Value Function\n",
    "\n",
    "- approximate $v_\\pi(s)$ as $\\hat{v}(s,\\mathbf{w})$\n",
    "\n",
    "$$\n",
    "\\hat{v}(s,\\underbrace{\\mathbf{w}}_{\\text{weights}}) \\approx v_\\pi(s)\n",
    "$$\n",
    "\n",
    "- Example of Parameterized Value Function\n",
    "\n",
    "$$\n",
    "\\hat{v}(s,\\mathbf{w}) \\doteq \\underbrace{w_1 X + w_2 Y}_{\\text{We only have to store the two weights}}\n",
    "$$\n",
    "\n",
    "## Linear Value Function Approximation\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{v}(s,\\mathbf{w}) &\\doteq \\sum{\\underbrace{w_i x_i(s)}_{\\text{features}}}\\\\\n",
    "                      &=<\\mathbf{w},\\mathbf{x}(s)>\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization and Discrimination\n",
    "\n",
    "- Tabular representations have provided good *discrimination*, but no *generalization*\n",
    "- Generalization is important for faster learning\n",
    "- Having both generalization and discrimination is ideal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing Value Estimation as Suprevised Learning\n",
    "\n",
    "- Monte-Carlo\n",
    "\n",
    "$$\n",
    "\\{(S_1,G_1),(S_2,G_2),(S_3,G_3),\\ldots\\}\n",
    "$$\n",
    "\n",
    "- TD\n",
    "\n",
    "$$\n",
    "\\{(S_1,R_2+\\gamma\\hat{v}(S_2,\\mathbf{w})),(S_2,R_3+\\gamma\\hat{v}(S_3,\\mathbf{w})),(S_3,R_4+\\gamma\\hat{v}(S_4,\\mathbf{w})),\\ldots\\}\n",
    "$$\n",
    "\n",
    "## The Function Approximator should be Compatible with Online Updates\n",
    "\n",
    "\n",
    "- We can frame the policy evaluation task as a *suprevised learning* problem\n",
    "- But not all methods from supervised learning are ideal for reinforcement learning\n",
    "  - If we want to use a function approximation technique, we should make sure it can work in the online setting\n",
    "  - The data in reinforcement learning is always correlated\n",
    "\n",
    "## The Function Approximator should be Compatible with Bootstrapping\n",
    "\n",
    "- TD: Target depends on $\\mathbf{w}$\n",
    "- Supervised Learning: Target is fixed and given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mean Squared Value Error Objective\n",
    "\n",
    "- Mean Squared Value Error\n",
    "\n",
    "$$\n",
    "\\sum_{s}{\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})]^2}\n",
    "$$\n",
    "\n",
    "- How to choose $\\mu(s)$?\n",
    "  - The fraction of time we spend in $S$ when following policy $\\pi$\n",
    "\n",
    "## Adapting the Weights to Minimize the Mean Squared Value Error Objective\n",
    "\n",
    "$$\n",
    "\\overline{VE}=\\sum_{s}{\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})]^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Gradient Descent\n",
    "\n",
    "- Gradient Descent\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t+1} \\doteq \\mathbf{w}_t - \\alpha \\nabla J(\\mathbf{w}_t)\n",
    "$$\n",
    "\n",
    "- Gradient Descent can be used to find stationary points of objectives\n",
    "- These solutions are not always globally optimal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of the Mean Squared Value Error Objective\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\nabla \\sum_{s\\in\\mathcal{S}}{\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})]^2}\\\\\n",
    "&= \\sum_{s\\in\\mathcal{S}}{\\nabla\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})]^2}\\\\\n",
    "&= -\\sum_{s\\in\\mathcal{S}}{\\mu(s)2[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})]} \\nabla \\hat{v}(s,\\mathbf{w})& \\text{(chain rule)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\Delta \\mathbf{w} \\propto \\sum_{s\\in\\mathcal{S}}{\\nabla\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})]}& (\\because\\Delta\\hat{v}(s,\\mathbf{w})=\\mathbf{x}(s))\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Gradient Descent to Stochastic Gradient Descent\n",
    "\n",
    "### Gradient Monte Carlo\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\mathbf{w_{t+1}}\\doteq\\mathbf{w_t}+\\alpha[\\underbrace{v_\\pi(S_t)}_{\\text{?}}-\\hat{v}(S_t,\\mathbf{w})\\nabla\\hat{v}(S_t,\\mathbf{w})]\\\\\n",
    "\\mathbf{w_{t+1}}\\doteq\\mathbf{w_t}+\\alpha[G_t-\\hat{v}(S_t,\\mathbf{w})\\nabla\\hat{v}(S_t,\\mathbf{w})]\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\mathbb{E}_\\pi[2[v_\\pi(S_t)-\\hat{v}(S_t,\\mathbf{w})]\\nabla\\hat{v}(S_t,\\mathbf{w})]\\\\\n",
    "=\\mathbb{E}_\\pi[2[G_t-\\hat{v}(S_t,\\mathbf{w})]\\nabla\\hat{v}(S_t,\\mathbf{w})]\n",
    "\\end{gather*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Aggregation\n",
    "\n",
    "- State aggregation treats certain states as the same\n",
    "- State aggregation is another example of linear function approximation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Gradient TD for Policy Evaluation\n",
    "\n",
    "### The TD Update for Function Approximation\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [U_t - \\hat{v}(S_t, \\mathbf{w})]\\nabla \\hat{v}(S_t, \\mathbf{w})\\\\\n",
    "U_t \\doteq R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "- $U_t$: biased $\\rightarrow$ $\\mathbf{w}$ may not converge to a local optimum\n",
    "\n",
    "### TD is a semi-gradient method\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla \\frac{1}{2} [U_t - \\hat{v}(S_t, \\mathbf{w})]^2 &= (U_t - \\hat{v}(S_t, \\mathbf{w})) (\\nabla U_t - \\nabla\\hat{v}(S_t, \\mathbf{w})) \\\\\n",
    "                                                      &\\neq \\underbrace{- (U_t - \\hat{v}(S_t, \\mathbf{w}))\\nabla\\hat{v}(S_t, \\mathbf{w})}_{\\text{The TD Update}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For TD:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla U_t &= \\nabla (R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}))\\\\\n",
    "           &= \\gamma \\nabla\\hat{v}(S_t, \\mathbf{w})\\\\\n",
    "           &\\neq 0\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing TD and Monte Carlo with State Aggregation\n",
    "\n",
    "- The TD update for function approximation can be *biased*\n",
    "- We often prefer TD learning over Monte Carlo anyway because it can converge more quickly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear TD Update\n",
    "\n",
    "- recall: semi gradient TD\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w} &\\leftarrow \\mathbf{w} + \\alpha \\delta_t \\nabla \\hat{v}(S_t, \\mathbf{w})&  (\\delta_t \\doteq R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})-\\hat{v}(S_{t}, \\mathbf{w})) \\\\\n",
    "\\mathbf{w} &\\leftarrow \\mathbf{w} + \\alpha \\delta_t \\mathbf{x}(S_t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Tabular TD is a special case of linear TD\n",
    "\n",
    "### The Utility of Linear Function Approximation\n",
    "\n",
    "- Linear methods are *simpler to understand and analyze* mathematically\n",
    "- With *good features*, linear methods can learn quickly and achieve good prediction accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The True Objective for TD\n",
    "\n",
    "### The Expected TD Update\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w}_{t+1} &\\doteq \\mathbf{w}_t + \\alpha [R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}_t)-\\hat{v}(S_t, \\mathbf{w}_t)]\\mathbf{x}_t & (\\hat{v}(s, \\mathbf{w})\\doteq \\mathbf{w}^T\\mathbf{x}(s))\\\\\n",
    "                 &= \\mathbf{w}_t + \\alpha [R_{t+1} + \\gamma \\mathbf{w}^T_{t}\\mathbf{x}_{t+1}-\\mathbf{w}^T_{t}\\mathbf{x}_{t}]\\mathbf{x}_t \\\\\n",
    "                 &= \\mathbf{w}_t + \\alpha [\\underbrace{R_{t+1}\\mathbf{x}_t}_{\\mathbf{b}} - \\underbrace{\\mathbf{x}_{t}(\\mathbf{x}_{t}-\\gamma \\mathbf{x}_{t+1})^T}_{\\mathbf{A}}\\mathbf{w}_t]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "&\\mathbb{E}[\\Delta\\mathbf{w}_t] = \\alpha(\\mathbf{b}-\\mathbf{A}\\mathbf{w}_t)&(\\mathbf{b}\\doteq \\mathbb{E}[R_{t+1}\\mathbf{x}_t],\\ \\mathbf{A}\\doteq\\mathbb{E}[\\mathbf{x}_t(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1})^T])\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "### The TD Fixed Point\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\mathbb{E}[\\Delta \\mathbf{w}_{TD}] = \\alpha (\\mathbf{b}-\\mathbf{A}\\mathbf{w}_{TD}) = 0\\\\\n",
    "\\Rightarrow \\mathbf{w}_{TD} = \\mathbf{A}^{-1}\\mathbf{b} \\\\\n",
    "\\mathbf{w}_{TD}\\text{ minimizes }(\\mathbf{b}-\\mathbf{A}\\mathbf{w})^T(\\mathbf{b}-\\mathbf{A}\\mathbf{w})\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "### Relating the TD Fixed Point and the Minimum of the Value Error\n",
    "\n",
    "$$\n",
    "\\overline{VE}(\\mathbf{w}_{TD})\\leq\\frac{1}{1-\\gamma}\\min_{\\mathbf{w}}\\overline{VE}(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Linear semi-gradient TD is guaranteed to converge to a fixed point, called the *TD fixed point*\n",
    "- The TD fixed point relates to the minimum mean squared value error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}