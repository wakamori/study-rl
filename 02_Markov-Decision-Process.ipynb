{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "\n",
    "- MDPs provide a general framework for sequential decision-making\n",
    "- The dynamics of an MDP are defined by a following probability distribution\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "p(s',r|s,a) \\\\\n",
    "p:\\mathcal{S}\\times\\mathcal{R}\\times\\mathcal{S}\\times\\mathcal{A} \\rightarrow [0, 1] \\\\\n",
    "\\sum_{s'\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{R}}{p(s',r|s,a)}=1,\\ \\forall{S}\\in\\mathcal{S},a\\in\\mathcal{A}(s)\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of an Agent\n",
    "\n",
    "- The *goal of an agent* is to *maximize the expected return*\n",
    "- In *episodic tasks*, the agent-environment interaction breaks up into *episodes*\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[G_t] \\doteq \\mathbb{E}[R_{t+1}+R_{t+2}+R_{t+3}+\\ldots+\\underbrace{R_T}_{\\text{final time step}}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic Tasks and Continuing Tasks\n",
    "\n",
    "- Episodic Tasks\n",
    "  - Interaction breaks naturally into *episodes*\n",
    "  - Each episode endgs in a *terminal state*\n",
    "  - Episodes are *independent*\n",
    "- Continuing Tasks\n",
    "  - Interaction goes on *continually*\n",
    "  - No *terminal state*\n",
    "\n",
    "### Discounting\n",
    "\n",
    "- To make sure $G_t$ is finite, *discount* the rewards in the future by $\\gamma$ where $0 \\leq \\gamma \\lt 1$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t \&\doteq R_{t+1}+\\gamma R_{t+2} +\\gamma^2 R_{t+3}+\\ldots+\\gamma^{k-1}R_{t+k}+\\ldots\\\\\n",
    "         &= \\underbrace{\\sum^{\\infty}_{k=0}{\\gamma^kR_{t+k+1}}}_{\\text{Finit as long as }0 \\leq \\gamma \\lt}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Assume $R_{max}$ is the maximum reward the agent can receive\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t=\\sum^{\\infty}_{k=0}\\gamma^kR_{t+k+1}\\leq\\sum^{\\infty}_{k=0}\\gamma^kR_{max}&=R_{max}\\sum^{\\infty}_{k=0}\\gamma^k\\\\\n",
    "                                                                              &=\\underbrace{R_{max}\\times\\frac{1}{1-\\gamma}}_{\\text{Finite}\\Rightarrow G_t\\text{ is finite}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- If $\\gamma = 0$, agent only cares about the immediate reward $\\Rightarrow$ *short-sighted agent*\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t \&\doteq R_{t+1}+\\gamma R_{t+2} +\\gamma^2 R_{t+3}+\\ldots+\\gamma^{k-1}R_{t+k}+\\ldots\\\\\n",
    "         &= R_{t+1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- If $\\gamma \\rightarrow 1$, agent takes future rewards into account more strongly $\\Rightarrow$ *Far-sighted agent*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive nature of returns\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t \&\doteq R_{t+1}+\\gamma R_{t+2} +\\gamma^2 R_{t+3}+\\ldots+\\gamma^{k-1}R_{t+k}+\\ldots\\\\\n",
    "         &= R_{t+1}+\\gamma(R_{t+2}+\\gamma R_{t+3}+\\gamma^2R_{t+4}+\\ldots)\\\\\n",
    "         &= R_{t+1}+\\gamma G_{t+1}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ]
}
