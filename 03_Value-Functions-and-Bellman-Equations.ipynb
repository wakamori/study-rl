{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Functions & Bellman Equations\n",
    "\n",
    "## Specifying Policies\n",
    "\n",
    "- A *policy* maps the current *state* onto a set of *probabilities for taking each action*\n",
    "- *Policies* can *only depend* on the current *state*\n",
    "\n",
    "- Deterministic policy notation\n",
    "\n",
    "$$\n",
    "\\pi(s) = a\n",
    "$$\n",
    "\n",
    "- Stochastic policy notation\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{gather*}\n",
    "\\pi(a|s)\\\\\n",
    "\\sum_{a\\in\\mathcal{A}(s)}{\\pi(a|s)}=1\\\\\n",
    "\\pi(a|s)\\geq 0\n",
    "\\end{gather*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Value functions\n",
    "\n",
    "- *State-value functions* represent the *expected return* from a *given state* under a *specific policy*\n",
    "- *Action-value functions* represent the *expected return from a *given state* after taking a *specific action*, later following a *specific policy*\n",
    "\n",
    "### State-value functions\n",
    "\n",
    "$$\n",
    "v_\\pi(s) \\doteq \\mathbb{E}_\\pi[G_t|S_t=s]\n",
    "$$\n",
    "\n",
    "### Action-value functions\n",
    "\n",
    "$$\n",
    "q_\\pi(s,a) \\doteq \\mathbb{E}_\\pi[G_t|S_t=s,A_t=a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation Derivation\n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "- Marginalization\n",
    "\n",
    "$$\n",
    "p(a)=\\sum_{b}p(a,b) \\tag{1}\n",
    "$$\n",
    "\n",
    "- Multiplication Theorem\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "p(a,b)=p(a|b)p(b) \\\\\n",
    "p(a,b|c)=p(a|b,c)p(b|c) \\tag{2}\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "- Conditional Expectation\n",
    "\n",
    "$$\n",
    "\\sum_a{\\mathbb{E}[Y|a]p(a)} = \\mathbb{E}[Y] \\tag{3}\n",
    "$$\n",
    "\n",
    "- Law of total Expectation\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X]=\\mathbb{E}\\left[\\mathbb{E}[X|Y]\\right] \\tag{4}\n",
    "$$\n",
    "\n",
    "- Transition probability\n",
    "\n",
    "$$\n",
    "\\pi(a|s)=p(a|s) \\tag{5}\n",
    "$$\n",
    "\n",
    "- Expected value of immediate reward\n",
    "\n",
    "$$\n",
    "r=r(s,a,s')=E_\\pi[R_{t+1}|S_t=s,A_t=a,S_{t+1}=s'] \\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-value Bellman Equation\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma v_\\pi(s')\\right],\\ \\forall{s\\in\\mathcal{S}}\n",
    "$$\n",
    "\n",
    "#### Derivation\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_\\pi(s) &\\doteq \\mathbb{E}_\\pi[G_t|S_t=s] \\\\\n",
    "         &= \\mathbb{E}_\\pi[R_{t+1}+\\gamma G_{t+1}|S_t=s] \\\\\n",
    "         &= \\sum_{a}\\pi(a|s)\\sum_{s'}p(s'|s,a)\\mathbb{E}_\\pi[R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a,S_{t+1}=s'] & \\text{from (3)}\\\\\n",
    "         &= \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\mathbb{E}_\\pi[R_{t+1}+\\gamma G_{t+1}|\\underbrace{S_t=s,A_t=a}_{\\text{ignorable}},S_{t+1}=s'] & \\text{from (1)}\\\\\n",
    "         &= \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma\\mathbb{E}_\\pi[G_{t+1}|S_{t+1}=s']] & \\text{from (6)}\\\\\n",
    "         &= \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma v_\\pi(s')\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Action-value Bellman Equation\n",
    "\n",
    "$$\n",
    "q_\\pi(s,a) = \\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right],\\ \\forall{s\\in\\mathcal{S}},\\ \\forall{a\\in\\mathcal{A}}\n",
    "$$\n",
    "\n",
    "#### Derivation\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_\\pi(s,a) &\\doteq \\mathbb{E}_\\pi[G_t|S_t=s,A_t=a] \\\\\n",
    "           &= \\sum_{s',r}p(s',r|s,a)\\mathbb{E}_\\pi[G_t|S_{t+1}=s'] \\\\\n",
    "           &= \\sum_{s',r}p(s',r|s,a)\\mathbb{E}_\\pi[R_{t+1}+\\gamma G_{t+1}|S_{t+1}=s'] \\\\\n",
    "           &= \\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma\\sum_{a'}\\pi(a'|s')\\mathbb{E}_\\pi[G_{t+1}|S_{t+1}=s',A_{t+1}=a']\\right] \\\\\n",
    "           &= \\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma\\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right] \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Bellman Equation\n",
    "\n",
    "- We can use the *Bellman Equation* to solve for a *value function* by writing a *system of linear equatinos*\n",
    "- We can only solve *small MDPs* directly, but *Bellman Equations* will factor into the solutions we see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policy\n",
    "\n",
    "- An *optimal policy* is defined as the policy with the *highest possible value function* in *all states*\n",
    "- *At least* one optimal policy always exists, but there may be more than one\n",
    "- The *exponential number* of possible policies makes searching for the optimal policy by brute-force intractable\n",
    "\n",
    "$$\n",
    "v_\\star(s)\\doteq\\max_{\\pi}v_\\pi(s),\\ \\forall{s\\in\\mathcal{S}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_\\star(s,a) &\\doteq \\max_{\\pi}q_\\pi(s,a),\\ \\forall{s\\in\\mathcal{S}},\\ \\forall{a\\in\\mathcal{A}}\\\\\n",
    "             &=\\mathbb{E}[R_{t+1}+\\gamma v_\\star(S_{t+1})|S_t=s,A_t=a]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Bellman Optimality Equation\n",
    "\n",
    "- Bellman Optimality Equation for $v_\\star$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_\\star(s) &= \\max_{a}q_{\\pi_\\star}(s,a) \\\\\n",
    "           &= \\max_{a}\\mathbb{E}_{\\pi_\\star}[G_t|S_t=s,A_t=a] \\\\\n",
    "           &= \\max_{a}\\mathbb{E}_{\\pi_\\star}[R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a] \\\\\n",
    "           &= \\max_{a}\\mathbb{E}[R_{t+1}+\\gamma v_\\star(S_{t+1})|S_t=s,A_t=a] \\\\\n",
    "            \n",
    "           &= \\max_{a}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_\\star(s')]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Bellman Optimality Equation for $q_\\star$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_\\star(s,a) &=\\mathbb{E}[R_{t+1}+\\gamma \\max_{a'}v_\\star(S_{t+1},a')|S_t=s,A_t=a] \\\\\n",
    "             &=\\sum_{s',r}p(s',r|s,a)[r+\\gamma \\max_{a'}q_\\star(s',a')]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining an Optimal Policy\n",
    "\n",
    "- from optimal state-value function\n",
    "\n",
    "$$\n",
    "\\pi_\\star(s)=\\underset{a}{\\mathrm{argmax}}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_\\star(s')]\n",
    "$$\n",
    "\n",
    "- from optimal action-value function\n",
    "\n",
    "$$\n",
    "\\pi_\\star(s)=\\underset{a}{\\mathrm{argmax}}\\ q_\\star(s,a)\n",
    "$$"
   ]
  }
 ]
}