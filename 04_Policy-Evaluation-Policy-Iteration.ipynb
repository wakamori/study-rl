{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation (Prediction) vs. Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "- *Policy Evaluation* is the task of determining the state-value function $v_\\pi$, for a particular policy $\\pi$\n",
    "\n",
    "$$\n",
    "\\pi,p,\\gamma \\rightarrow \\text{(Dynamic Programming)} \\rightarrow v_\\pi\n",
    "$$\n",
    "\n",
    "## Control\n",
    "\n",
    "- *Control* is the task of improving an existing policy\n",
    "\n",
    "$$\n",
    "p,\\gamma \\rightarrow \\text{(Dynamic Programming)} \\rightarrow \\pi_\\star\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation\n",
    "\n",
    "$$\n",
    "v_{k+1}(s) \\leftarrow \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r + \\gamma v_k(s')]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "- The new policy is a strict improvement over $\\pi$ unless $\\pi$ is already optimal\n",
    "\n",
    "## Policy Improvement theorem\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "q_\\pi\\left(s,\\pi'(s)\\right) \\geq q_\\pi\\left(s,\\pi(s)\\right),\\ \\forall s\\in\\mathcal{S} \\rightarrow \\pi' \\geq \\pi \\\\\n",
    "q_\\pi\\left(s,\\pi'(s)\\right) \\gt q_\\pi\\left(s,\\pi(s)\\right),\\text{ for all at least one }s\\in\\mathcal{S} \\rightarrow \\pi' \\gt \\pi\n",
    "\\end{gather*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "- *Policy Iteration* works by alternating *policy evaluation* and *policy improvement*\n",
    "- Policy Iteration follows a sequence of *better and better policies and value functions* until it reaches the optimal policy and associated optimal value functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "- *Value Iteration* allows us to combine policy evaluation and improvement into a single update\n",
    "- *Asynchronous* dynamic programming methods give us the freedom to update states in any order\n",
    "- *Generalized Policy Iteration* unifies classical DP methods, value iteration, and asynchronous DP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency of Dynamic Programming\n",
    "\n",
    "- Policy Iteration: Polinomial time in $|\\mathcal{S}|$ and $|\\mathcal{A}|$\n",
    "- Brute-Force Search: $|\\mathcal{A}|^{|\\mathcal{S}|}$ policies\n",
    "- *Bootstrapping* can save us from performing a huge amount of unnecessary work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}