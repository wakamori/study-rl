{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Policies Directly\n",
    "\n",
    "### Constraints on the Policy Parameterization\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\pi(a|s,\\theta) \\geq 0 & \\forall a \\in \\mathcal{A},s \\in \\mathcal{S}\\\\\n",
    "\\sum_{a\\in\\mathcal{A}}{\\pi(a|s,\\theta)=1} & \\forall s \\in \\mathcal{S}\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "### The Softmax Policy Parameterization\n",
    "\n",
    "$$\n",
    "\\pi(a|s,\\theta)\\doteq\\frac{\\underbrace{e^{h(s,a,\\theta)}}{\\text{Action Preference}}}{\\sum_{b\\in\\mathcal{A}}{e^{h(s,b,\\theta)}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of Policy Parameterization\n",
    "\n",
    "### Parameterized stochastic policies are useful because\n",
    "\n",
    "- They can autonomously *decrease exploration* over time\n",
    "- They can avoid failures due to deterministic policies with *limited function approximation*\n",
    "- Sometimes the policy is less complicated than the value function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Objective for Learning Policies\n",
    "\n",
    "### The Average Reward Objective\n",
    "\n",
    "$$\n",
    "r(\\pi)=\\underbrace{\\sum_{s}\\mu(s)\\underbrace{\\sum_{a}\\pi(a|s,\\theta)\\underbrace{\\sum_{s',r}p(s',r|s,a)r}_{\\mathbb{E}[R_t|S_t=s,A_t=a]}}_{\\mathbb{E}_\\pi[R_t|S_t=s]}}_{\\mathbb{E}_\\pi[R_t]}\n",
    "$$\n",
    "\n",
    "### Optimizing The Average Reward Objective\n",
    "\n",
    "- Policy-Gradient Method\n",
    "\n",
    "$$\n",
    "\\nabla r(\\pi)=\\nabla\\sum_{s}\\mu(s)\\sum_{a}\\pi(a|s,\\theta)\\sum_{s',r}p(s',r|s,a)r\n",
    "$$\n",
    "\n",
    "### The Challenge of Policy Gradient Methods\n",
    "\n",
    "- We can use the average reward as an objective for policy optimization\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta r(\\pi)=\\nabla_\\theta\\sum_{s}\\underbrace{\\mu(s)}_{\\text{Depends on }\\theta}\\sum_{a}\\pi(a|s,\\theta)\\sum_{s',r}p(s',r|s,a)r\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_\\mathbf{w}\\overline{VE} &= \\nabla_\\mathbf{w}\\sum_{s}\\mu(s)[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})]^2 \\\\\n",
    "                               &= \\sum_{s}\\mu(s)\\nabla_\\mathbf{w}[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})]^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Policy Gradient Theorem\n",
    "\n",
    "### The Gradient of the Objective\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla r(\\pi) &= \\nabla\\sum_{s}\\mu(s)\\sum_{a}\\pi(a|s,\\theta)\\sum_{s',r}p(s',r|s,a)r \\\\\n",
    "              &= \\sum_{s}\\nabla\\mu(s)\\sum_{a}\\pi(a|s,\\theta)\\sum_{s',r}p(s',r|s,a)r + \\sum_{s}\\mu(s)\\nabla\\sum_{a}\\pi(a|s,\\theta)\\sum_{s',r}p(s',r|s,a)r\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### The Policy Gradient Theorem\n",
    "\n",
    "- The *policy gradient theorem* gives an expression for the gradient of the average reward\n",
    "\n",
    "$$\n",
    "\\nabla r(\\pi) = \\sum_{s}\\mu(s)\\sum_{a}\\nabla \\pi(a|s,\\theta)q_\\pi(s,a)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Policy Gradient\n",
    "\n",
    "### Getting Stochastic Samples of the Gradient\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\nabla r(\\pi)=\\sum_{s}\\mu(s)\\sum_{a}\\nabla \\pi(a|s,\\theta)q_\\pi(s,a)\\\\\n",
    "\\theta_{t+1}\\doteq\\theta_{t}+\\alpha\\sum_{a}\\nabla\\pi(a|S_t,\\theta_t)q_\\pi(S_t,a)\\\\\n",
    "\\\\\n",
    "S_0,A_0,R_1,S_1,A_1,\\ldots,S_t,A_t,R_{t+1},\\ldots\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "### Unbiasedness of the Stochastic Samples\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla r(\\pi) &=\\sum_{s}\\mu(s)\\sum_{a}\\nabla\\pi(a|s,\\theta)q_\\pi(s,a)\\\\\n",
    "              &=\\mathbb{E}_\\mu[\\sum_{a}\\nabla\\pi(a|S,\\theta)q_\\pi(S,a)]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Getting Stochastic Samples with One Action\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\sum_{a}\\nabla\\pi(a|S,\\theta)q_\\pi(S,a)\\\\\n",
    "&=\\sum_{a}\\pi(a|S,\\theta)\\frac{1}{\\pi(a|S,\\theta)}\\nabla\\pi(a|S,\\theta)q_\\pi(S,a)\\\\\n",
    "&=\\mathbb{E}_\\pi[\\frac{\\nabla\\pi(A|S,\\theta)}{\\pi(A|S,\\theta)}q_\\pi(S,A)]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Stochastic Gradient Ascent for Policy Parameters\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta_{t+1}&\\doteq\\theta_{t}+\\alpha\\frac{\\nabla\\pi(A_t|S_t,\\theta_t)}{\\pi(A_t|S_t,\\theta_t)}q_\\pi(S_t,A_t)\\\\\n",
    "            &=\\theta_{t}+\\alpha\\nabla\\ln\\pi(A_t|S_t,\\theta_t)q_\\pi(S_t,A_t)&(\\because \\nabla\\ln\\left(f(x)\\right)=\\frac{\\nabla f(x)}{f(x)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Computing the Update\n",
    "\n",
    "$$\n",
    "\\theta_{t+1}=\\theta_{t}+\\alpha\\underbrace{\\nabla\\ln\\pi(A_t|S_t,\\theta_t)}_{\\text{gradient of the policy (computable)}}\\underbrace{q_\\pi(S_t,A_t)}_{\\text{estimate of the differntial valus (computable)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}